{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec90acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import random\n",
    "\n",
    "\n",
    "class DQNAgent(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.first_layer = params[\"first_layer_size\"]\n",
    "        self.second_layer = params[\"second_layer_size\"]\n",
    "        self.third_layer = params[\"third_layer_size\"]\n",
    "        self.weights = params[\"weights_path\"]\n",
    "        self.load_weights = params['load_weights']\n",
    "        self.memory = collections.deque(maxlen=params['memory_size'])\n",
    "        self.learning_rate = params[\"learning_rate\"]\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1\n",
    "        self.agent_target = 1\n",
    "        self.agent_predict = 0\n",
    "        self.optimizer = None\n",
    "\n",
    "        self.network()\n",
    "\n",
    "    def hot_encode(self, cards, length=54):\n",
    "        try:\n",
    "            tensor = torch.zeros(length)\n",
    "            tensor[cards] = 1.0\n",
    "            return tensor\n",
    "        except IndexError:\n",
    "            print(cards)\n",
    "            tensor = torch.zeros(length)\n",
    "            raise ValueError(\"Let see this index error thingy\")\n",
    "            return tensor\n",
    "\n",
    "    def network(self):\n",
    "        self.f1 = nn.Linear(117, self.first_layer)\n",
    "        self.f2 = nn.Linear(self.first_layer, self.second_layer)\n",
    "        self.f3 = nn.Linear(self.second_layer, self.third_layer)\n",
    "        self.f4 = nn.Linear(self.third_layer, 60)\n",
    "\n",
    "        if self.load_weights:\n",
    "            self.model = self.load_state_dict(torch.load(self.weights))\n",
    "            print(\"weights loaded\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.f1(x))\n",
    "        x = F.relu(self.f2(x))\n",
    "        x = F.relu(self.f3(x))\n",
    "        x = F.softmax(self.f4(x), dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # def set_reward(self, closer, outcome):\n",
    "    #     self.reward = 0\n",
    "    #     if closer != 0:\n",
    "    #         self.reward = closer\n",
    "\n",
    "    #     if outcome == 'win':\n",
    "    #         self.reward = 10\n",
    "    #     if outcome == 'lose':\n",
    "    #         self.reward == -10\n",
    "\n",
    "    #     return self.reward\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_new(self, memory, batch_size):\n",
    "        if len(memory) > batch_size:\n",
    "            minibatch = random.sample(memory, batch_size)\n",
    "        else:\n",
    "            minibatch = memory\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            self.train()\n",
    "            torch.set_grad_enabled(True)\n",
    "\n",
    "            target = reward\n",
    "            next_state_tensor = torch.tensor(np.expand_dims(\n",
    "                next_state, 0), dtype=torch.float32, requires_grad=True)\n",
    "            state_tensor = torch.tensor(np.expand_dims(\n",
    "                state, 0), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "            if not done:\n",
    "                target = reward + self.gamma * \\\n",
    "                    torch.max(self.forward(next_state_tensor[0]))\n",
    "\n",
    "            output = self.forward(state_tensor)\n",
    "            target_f = output.clone()\n",
    "            target_f[0][np.argmax(action)] = target\n",
    "            target_f.detach()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = F.mse_loss(output, target_f)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        target = reward\n",
    "        next_state_tensor = torch.tensor(np.expand_dims(\n",
    "            next_state, 0), dtype=torch.float32, requires_grad=True)\n",
    "        state_tensor = torch.tensor(np.expand_dims(\n",
    "            state, 0), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        if not done:\n",
    "            target = reward + self.gamma * \\\n",
    "                torch.max(self.forward(next_state_tensor[0]))\n",
    "\n",
    "        output = self.forward(state_tensor)\n",
    "        target_f = output.clone()\n",
    "        target_f[0][np.argmax(action)] = target\n",
    "        target_f.detach()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(output, target_f)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def simple_env(self, agent_hand, top_card, turn, cardless, action=-1):\n",
    "        hand = self.hot_encode(agent_hand)\n",
    "        top = self.hot_encode([top_card])\n",
    "        actions = torch.zeros(7)\n",
    "        turn_t = torch.tensor([turn], requires_grad=False)\n",
    "        cardless_t = torch.tensor([cardless], requires_grad=False)\n",
    "        if action != -1:\n",
    "            actions[action] = 1\n",
    "\n",
    "        env_space = torch.cat([hand, top, actions, cardless_t, turn_t], dim=0)\n",
    "        return env_space\n",
    "\n",
    "    # def get_env_space(self, agent_hand, top_card, wastes, deck_size, actions, player_turn, poss_moves):\n",
    "        print(agent_hand, top_card, wastes, deck_size, actions, player_turn)\n",
    "        hand = hot_encode(agent_hand)\n",
    "        top = hot_encode([top_card])\n",
    "        waste = hot_encode(wastes)\n",
    "        deck = torch.zeros([55])\n",
    "        deck[0] = deck_size/54\n",
    "        action = hot_encode(actions)\n",
    "        player_turn = hot_encode(player_turn)\n",
    "\n",
    "        env_space = torch.stack([hand, top, waste, deck, action, player_turn])\n",
    "\n",
    "        env_space_sq = torch.cat(\n",
    "            [env_space.unsqueeze(dim=0), torch.zeros(1, 10, 55)], dim=1)\n",
    "        action_space = torch.zeros(32, 16, 55)\n",
    "\n",
    "        print(\"size\", env_space_sq.size())\n",
    "        action_space[31, 0, 54] = 1.0\n",
    "\n",
    "        for i, move in enumerate(poss_moves):\n",
    "            for j, card in enumerate(poss_moves[i]):\n",
    "                action_space[i, j, card] = 1.0\n",
    "\n",
    "        combined_state = torch.cat([env_space_sq, action_space], dim=0)\n",
    "        return combined_state\n",
    "\n",
    "    # def get_action_space(self, poss_moves):\n",
    "\n",
    "    #     return action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hands\n",
      "['2_of_clubs', '6_of_diamonds', '3_of_hearts', 'queen_of_diamonds']\n",
      "['5_of_spades', '4_of_spades', '4_of_diamonds', '4_of_hearts']\n",
      "\n",
      " top card\n",
      "10_of_diamonds\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 1 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 2 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 3 turn: 0\n",
      "player 1\n",
      "reward 1\n",
      "game: 0.  step: 4 turn: 0\n",
      "player 1\n",
      "reward 0\n",
      "game: 0.  step: 5 turn: 1\n",
      "player 2\n",
      "reward 0\n",
      "game: 0.  step: 6 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 7 turn: 0\n",
      "player 1\n",
      "reward 1\n",
      "game: 0.  step: 8 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 9 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 10 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 11 turn: 0\n",
      "player 1\n",
      "reward 3\n",
      "game: 0.  step: 12 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 13 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 14 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 15 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 16 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 17 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 18 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 19 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 20 turn: 1\n",
      "player 2\n",
      "reward 0\n",
      "game: 0.  step: 21 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 22 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 23 turn: 0\n",
      "player 1\n",
      "reward 1\n",
      "game: 0.  step: 24 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 25 turn: 0\n",
      "player 1\n",
      "reward 3\n",
      "game: 0.  step: 26 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 27 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 28 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 29 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 30 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 31 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 32 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 33 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 34 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 35 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 36 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 37 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 38 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 39 turn: 1\n",
      "player 2\n",
      "reward 1\n",
      "game: 0.  step: 40 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 41 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 42 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 43 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 44 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 45 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 46 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 47 turn: 1\n",
      "player 2\n",
      "reward 3\n",
      "game: 0.  step: 48 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 49 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 50 turn: 0\n",
      "player 1\n",
      "reward 0\n",
      "game: 0.  step: 51 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 52 turn: 1\n",
      "player 2\n",
      "reward 0\n",
      "game: 0.  step: 53 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 54 turn: 0\n",
      "player 1\n",
      "reward 0\n",
      "game: 0.  step: 55 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 56 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 57 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 58 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 59 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 60 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 61 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 62 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 63 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 64 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 65 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 66 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 67 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 68 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 69 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 70 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 71 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 72 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 73 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 74 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 75 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 76 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 77 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 78 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 79 turn: 1\n",
      "player 2\n",
      "reward -1.5\n",
      "game: 0.  step: 80 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 81 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 82 turn: 0\n",
      "player 1\n",
      "reward 0\n",
      "game: 0.  step: 83 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 84 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 85 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 86 turn: 1\n",
      "player 2\n",
      "reward -2.0\n",
      "game: 0.  step: 87 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 88 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 89 turn: 0\n",
      "player 1\n",
      "reward 1\n",
      "game: 0.  step: 90 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 91 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 92 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 93 turn: 0\n",
      "player 1\n",
      "reward 3\n",
      "game: 0.  step: 94 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 95 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 96 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 97 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 98 turn: 1\n",
      "player 2\n",
      "reward -2.5\n",
      "game: 0.  step: 99 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 100 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 101 turn: 0\n",
      "player 1\n",
      "reward 0\n",
      "game: 0.  step: 102 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 103 turn: 1\n",
      "player 2\n",
      "reward 1\n",
      "game: 0.  step: 104 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 105 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 106 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 107 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 108 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 109 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 110 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 111 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 112 turn: 1\n",
      "player 2\n",
      "reward 1\n",
      "game: 0.  step: 113 turn: 1\n",
      "player 2\n",
      "reward 3\n",
      "game: 0.  step: 114 turn: 0\n",
      "player 1\n",
      "reward 1\n",
      "game: 0.  step: 115 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 116 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 117 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 118 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 119 turn: 0\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.]])\n",
      "player 1\n",
      "reward -3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54827/4136375265.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(env_space.clone().detach().reshape(1, 117), dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game: 0.  step: 120 turn: 0\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.]])\n",
      "player 1\n",
      "reward -3\n",
      "game: 0.  step: 121 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 122 turn: 0\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.]])\n",
      "player 1\n",
      "reward -3\n",
      "game: 0.  step: 123 turn: 0\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.]])\n",
      "player 1\n",
      "reward -3\n",
      "game: 0.  step: 124 turn: 0\n",
      "player 1\n",
      "reward 3\n",
      "game: 0.  step: 125 turn: 1\n",
      "player 2\n",
      "reward -2.0\n",
      "game: 0.  step: 126 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 127 turn: 0\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.]])\n",
      "player 1\n",
      "reward -3\n",
      "game: 0.  step: 128 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 129 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 130 turn: 0\n",
      "player 1\n",
      "reward 0\n",
      "game: 0.  step: 131 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 132 turn: 1\n",
      "player 2\n",
      "reward 1\n",
      "game: 0.  step: 133 turn: 1\n",
      "player 2\n",
      "reward 1\n",
      "game: 0.  step: 134 turn: 1\n",
      "player 2\n",
      "reward 3\n",
      "game: 0.  step: 135 turn: 1\n",
      "player 2\n",
      "reward 0\n",
      "game: 0.  step: 136 turn: 0\n",
      "player 1\n",
      "reward 1\n",
      "game: 0.  step: 137 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 138 turn: 0\n",
      "player 1\n",
      "reward 0\n",
      "game: 0.  step: 139 turn: 1\n",
      "player 2\n",
      "reward 1\n",
      "game: 0.  step: 140 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 141 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 142 turn: 1\n",
      "player 2\n",
      "reward 3\n",
      "game: 0.  step: 143 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 144 turn: 0\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.]])\n",
      "player 1\n",
      "reward -3\n",
      "game: 0.  step: 145 turn: 0\n",
      "player 1\n",
      "reward 1\n",
      "game: 0.  step: 146 turn: 0\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.]])\n",
      "player 1\n",
      "reward -3\n",
      "game: 0.  step: 147 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 148 turn: 0\n",
      "player 1\n",
      "reward 3\n",
      "game: 0.  step: 149 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 150 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 151 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 152 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 153 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 154 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 155 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 156 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 157 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 158 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 159 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 160 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 161 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 162 turn: 1\n",
      "player 2\n",
      "reward 0\n",
      "game: 0.  step: 163 turn: 0\n",
      "player 1\n",
      "reward 1\n",
      "game: 0.  step: 164 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 165 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 166 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 167 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 168 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 169 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 170 turn: 0\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.]])\n",
      "player 1\n",
      "reward -3\n",
      "game: 0.  step: 171 turn: 0\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.]])\n",
      "player 1\n",
      "reward -3\n",
      "game: 0.  step: 172 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 173 turn: 0\n",
      "player 1\n",
      "reward 3\n",
      "game: 0.  step: 174 turn: 1\n",
      "player 2\n",
      "reward 1\n",
      "game: 0.  step: 175 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 176 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 177 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 178 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 179 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 180 turn: 1\n",
      "player 2\n",
      "reward 3\n",
      "game: 0.  step: 181 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 182 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 183 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 184 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 185 turn: 0\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.]])\n",
      "player 1\n",
      "reward -3\n",
      "game: 0.  step: 186 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 187 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 188 turn: 0\n",
      "player 1\n",
      "reward -1\n",
      "game: 0.  step: 189 turn: 0\n",
      "player 1\n",
      "reward 0\n",
      "game: 0.  step: 190 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 191 turn: 1\n",
      "player 2\n",
      "reward 1\n",
      "game: 0.  step: 192 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 193 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 194 turn: 1\n",
      "player 2\n",
      "reward -1\n",
      "game: 0.  step: 195 turn: 1\n",
      "player 2\n",
      "reward 3\n",
      "game: 0.  step: 196 turn: 0\n",
      "player 1\n",
      "reward -1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from DQN import DQNAgent\n",
    "import torch.optim as optim\n",
    "from typing import Type\n",
    "from cards import cards as cs\n",
    "\n",
    "params = dict()\n",
    "\n",
    "params[\"first_layer_size\"] = 1024\n",
    "params[\"second_layer_size\"] = 512\n",
    "params[\"third_layer_size\"] = 256\n",
    "params[\"learning_rate\"] = 0.99\n",
    "params[\"memory_size\"] = 2500\n",
    "params[\"load_weights\"] = False\n",
    "params['train'] = True\n",
    "params[\"epsilon_decay_linear\"] = 0.001\n",
    "params[\"episodes\"] = 10\n",
    "params[\"batch_size\"] = 1000\n",
    "\n",
    "\n",
    "params1 = params.copy()\n",
    "params2 = params.copy()\n",
    "\n",
    "params1[\"weights_path\"] = \"weights/agent1/weights.h5\"\n",
    "params2[\"weights_path\"] = \"weights/agent2/weights.h5\"\n",
    "\n",
    "questions = [28, 29, 30, 31, 51, 50, 49, 48, 47, 46, 45, 44]\n",
    "aces = [0, 1, 2, 3]\n",
    "punishers = [4, 5, 6, 7, 8, 9, 10, 11, 52, 53]\n",
    "all_cards_without_jokers = list(range(52))\n",
    "\n",
    "\n",
    "def to_cs(n_l):\n",
    "    h = []\n",
    "    for n in n_l:\n",
    "        h.append(cs[n])\n",
    "    return h\n",
    "\n",
    "\n",
    "class Player():\n",
    "    def __init__(self, game, index) -> None:\n",
    "        self.hand = []\n",
    "        self.build = []\n",
    "        self.game = game\n",
    "        self.asking = False\n",
    "        self.index = index\n",
    "        self.reward = 0\n",
    "\n",
    "    def can_complete(self):\n",
    "        if len(self.build) == 0 or self.build[-1] in questions:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def waste_card(self, card):\n",
    "        white = self.game.white_list(build=self.build)\n",
    "        if card in white:\n",
    "            self.build += [card]\n",
    "            # print(f\"build:{ self.build} card: {card}\")\n",
    "            self.hand = list(filter(lambda x: x != card, self.hand))\n",
    "            self.reward  = 1\n",
    "        else:\n",
    "            # print(f\"build:{ self.build} card: {card}\")\n",
    "            self.reward = -1\n",
    "            # wrong move\n",
    "\n",
    "    def pick_cards(self, game):\n",
    "        game.pick(player=self)\n",
    "        self.game.new_turn()\n",
    "        self.build.clear()\n",
    "        self.reward = ((len(self.hand)-4) / 2)* -1 if len(self.hand)> 6 else 0\n",
    "\n",
    "    def complete_build(self, game):\n",
    "        if len(self.build) == 0:\n",
    "            self.reward = -1\n",
    "            return\n",
    "\n",
    "        game.waste(self)\n",
    "        \n",
    "\n",
    "        if len(self.hand) == 0 and self.build[-1] not in questions + aces + punishers and game.card_less == False:\n",
    "            self.reward = 10\n",
    "            game.complete = True\n",
    "            self.build = []\n",
    "            return\n",
    "        elif len(self.hand) == 0 and (self.build[-1] in questions+aces+punishers or game.card_less == True):\n",
    "            game.card_less = True\n",
    "\n",
    "        self.build.clear()\n",
    "        self.reward = 3\n",
    "        if not self.asking:\n",
    "            self.game.new_turn()\n",
    "\n",
    "    def choose_flower(self, flower, game):\n",
    "        if self.asking:\n",
    "            game.action = flower\n",
    "            game.new_turn()\n",
    "            self.asking = False\n",
    "        else:\n",
    "            self.reward = -1\n",
    "\n",
    "    def do_move(self, move, game):\n",
    "        self.reward = 0\n",
    "        if move < 54:\n",
    "            self.waste_card(move)\n",
    "        elif move > 53 and move < 58:\n",
    "            self.choose_flower(move-54, game)\n",
    "        elif move == 58:\n",
    "            self.complete_build(game)\n",
    "        elif move == 59:\n",
    "            self.pick_cards(game)\n",
    "\n",
    "\n",
    "class Game():\n",
    "    def __init__(self) -> None:\n",
    "        self.complete = True\n",
    "        self.wastes = []\n",
    "        self.deck = []\n",
    "        self.top_card = None\n",
    "        self.action = -1\n",
    "        self.turn = 0\n",
    "        self.card_less = False\n",
    "\n",
    "    def new_turn(self):\n",
    "        self.turn = (self.turn+1) % 2\n",
    "\n",
    "    def waste(self, player):\n",
    "        self.action = -1\n",
    "        top = player.build[-1]\n",
    "        if self.action > 3 and top < 4:\n",
    "            aces_in_build = list(filter(lambda x: x < 4, player.build))\n",
    "\n",
    "            if len(aces_in_build) < 2:\n",
    "                top = self.wastes[-1]\n",
    "                self.turn = (self.turn+1) % 2\n",
    "                self.wastes = self.wastes[0:-1] + player.build + self.wastes[-1:]\n",
    "            else:\n",
    "                player.asking = True\n",
    "        elif top < 4:\n",
    "            player.asking = True\n",
    "            self.wastes += player.build\n",
    "        else:\n",
    "            self.wastes += player.build\n",
    "\n",
    "        self.top_card = self.wastes[-1]\n",
    "\n",
    "        if self.top_card in punishers:\n",
    "            if math.floor(self.top_card/4) == 2:\n",
    "                self.action = 4\n",
    "            elif math.floor(self.top_card/4) == 3:\n",
    "                self.action = 5\n",
    "            elif self.top_card in [52, 53]:\n",
    "                self.action = 6\n",
    "\n",
    "    def pick(self, player):\n",
    "        pick_num = 1\n",
    "        if self.action == 4:\n",
    "            pick_num = 2\n",
    "            self.action = -1\n",
    "        elif self.action == 5:\n",
    "            pick_num = 3\n",
    "            self.action = -1\n",
    "        elif self.action == 6:\n",
    "            pick_num = 5\n",
    "            self.action = -1\n",
    "\n",
    "        if len(self.deck) < pick_num:\n",
    "            self.deck += self.wastes[0:-1]\n",
    "            self.wastes = self.wastes[-1:]\n",
    "            self.top_card = self.wastes[-1]\n",
    "\n",
    "        picked_cards = random.sample(self.deck, pick_num)\n",
    "        self.deck = list(filter(lambda x: x not in picked_cards, self.deck))\n",
    "\n",
    "        player.hand += picked_cards\n",
    "\n",
    "    def white_list(self, build=[]):\n",
    "        if self.action != -1:\n",
    "            if self.action < 4:\n",
    "                white = list(filter(lambda x: x %\n",
    "                             4 == self.action, all_cards_without_jokers))\n",
    "                if self.action < 2:\n",
    "                    white += [52]\n",
    "                else:\n",
    "                    white += [53]\n",
    "                return white + aces\n",
    "            elif self.action == 4:\n",
    "                return [4, 5, 6, 7] + aces\n",
    "            elif self.action == 5:\n",
    "                return [8, 9, 10, 11] + aces\n",
    "            elif self.action == 6:\n",
    "                return [52, 53] + aces\n",
    "\n",
    "        if len(build) == 0:\n",
    "            if self.top_card == 52:\n",
    "                white = list(filter(lambda x: x %\n",
    "                             4 < 2, all_cards_without_jokers))\n",
    "                white += aces + [52, 53]\n",
    "            elif self.top_card == 53:\n",
    "                white = list(filter(lambda x: x %\n",
    "                             4 > 1, all_cards_without_jokers))\n",
    "                white += aces + [52, 53]\n",
    "            else:\n",
    "                white = list(filter(lambda x: x % 4 == self.top_card % 4 or math.floor(\n",
    "                    x/4) == math.floor(self.top_card/4), all_cards_without_jokers))\n",
    "                if self.top_card % 4 < 2:\n",
    "                    white += [52]\n",
    "                else:\n",
    "                    white += [53]\n",
    "            return white\n",
    "        else:\n",
    "            last = build[-1]\n",
    "            if last in questions:\n",
    "                white = list(filter(lambda x: x % 4 == last % 4 or math.floor(\n",
    "                    x/4) == math.floor(last/4), all_cards_without_jokers))\n",
    "                if self.top_card % 4 < 2:\n",
    "                    white += [52]\n",
    "                else:\n",
    "                    white += [53]\n",
    "                return white + aces\n",
    "            elif last == 52 or last == 53:\n",
    "                return [52, 53]\n",
    "            else:\n",
    "                white = list(filter(lambda x: math.floor(\n",
    "                    x/4) == math.floor(last/4), all_cards_without_jokers))\n",
    "                return white\n",
    "\n",
    "\n",
    "def initialize_game(game, players):\n",
    "    game.deck = list(range(54))\n",
    "\n",
    "    for player in players:\n",
    "        player.hand = random.sample(game.deck, 4)\n",
    "\n",
    "        game.deck = list(filter(lambda x: x not in player.hand, game.deck))\n",
    "\n",
    "    poss_top = list(\n",
    "        filter(lambda x: x not in questions+aces+punishers, game.deck))\n",
    "    game.top_card = random.choice(poss_top)\n",
    "    game.wastes.append(game.top_card)\n",
    "\n",
    "    game.deck = list(filter(lambda x: x != game.top_card, game.deck))\n",
    "    game.complete = False\n",
    "\n",
    "\n",
    "game = Game()\n",
    "player1 = Player(game=game, index=0)\n",
    "player2 = Player(game=game, index=1)\n",
    "\n",
    "\n",
    "def play(player: Type[Player], agent: Type[DQNAgent], game: Type[Game], params: any, opponent: Type[Player], opp_agent: Type[DQNAgent]):\n",
    "    turn = 1\n",
    "    if (player.index != game.turn):\n",
    "        return\n",
    "\n",
    "    cardless = 1 if game.card_less == True else 0\n",
    "    env_space = agent.simple_env(\n",
    "        agent_hand=player.hand, top_card=game.top_card, turn=turn, cardless=cardless, action=game.action)\n",
    "\n",
    "    if random.uniform(0, 1) < agent.epsilon:\n",
    "        prediction = torch.rand(60)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(env_space.clone().detach().reshape(1, 117), dtype=torch.float32)\n",
    "            prediction = agent(state)\n",
    "            print(prediction)\n",
    "\n",
    "    if player.asking:\n",
    "        mask = torch.cat([torch.zeros(54), torch.tensor(\n",
    "            [1, 1, 1, 1], requires_grad=False), torch.zeros(2)], dim=0)\n",
    "    else:\n",
    "        complete = 1 if player.can_complete() else 0\n",
    "        asks = [0, 0, 0, 0]\n",
    "        pick = 0 if (len(player.build) >\n",
    "                     0 and player.build[-1] not in questions) else 1\n",
    "        mask = torch.cat([agent.hot_encode(player.hand), torch.tensor(\n",
    "            asks, requires_grad=False), torch.tensor([complete, pick], requires_grad=False)])\n",
    "\n",
    "    move = prediction.clone().detach()*mask\n",
    "\n",
    "    do_move = np.argmax(move).cpu().detach().numpy()\n",
    "\n",
    "    print(f\"player {1 if player == player1 else 2}\")\n",
    "    # print(f\"hand {to_cs(player.hand)}\")\n",
    "\n",
    "    if torch.all(torch.eq(move, 0)).item():\n",
    "        player.reward = -3\n",
    "    else:\n",
    "        player.do_move(move=do_move.item(), game=game)\n",
    "\n",
    "    if game.complete:\n",
    "        opponent.reward = -10\n",
    "        op_env = agent.simple_env(\n",
    "            agent_hand=opponent.hand, top_card=game.top_card, turn=turn, cardless=game.card_less, action=game.action\n",
    "        )\n",
    "\n",
    "        opp_agent.remember(state=op_env, action=torch.zeros(60),\n",
    "                           reward=opponent.reward, next_state=op_env, done=game.complete)\n",
    "        if params['train']:\n",
    "            opp_agent.replay_new(memory=opp_agent.memory,\n",
    "                                 batch_size=params['batch_size'])\n",
    "            model_weights = opp_agent.state_dict()\n",
    "            if player == player1:\n",
    "                torch.save(model_weights, params2[\"weights_path\"])\n",
    "            else:\n",
    "                torch.save(model_weights, params1[\"weights_path\"])\n",
    "\n",
    "    final_move = np.eye(60)[np.argmax(move).numpy()]\n",
    "\n",
    "    # print(f\"topCard: {cs[game.top_card]}\")\n",
    "    # print(f\"move:{cs[do_move]}\")\n",
    "    print(f\"reward {player.reward}\")\n",
    "    # print(f\"action:{game.action}\")\n",
    "\n",
    "    if (player.index != game.turn):\n",
    "        turn = 0\n",
    "\n",
    "    next_state = agent.simple_env(agent_hand=player.hand, top_card=game.top_card,\n",
    "                                  turn=turn, cardless=game.card_less, action=game.action)\n",
    "\n",
    "    agent.remember(state=env_space, action=final_move,\n",
    "                   reward=player.reward, next_state=next_state, done=game.complete)\n",
    "    if params['train']:\n",
    "        agent.replay_new(memory=agent.memory, batch_size=params['batch_size'])\n",
    "        model_weights = agent.state_dict()\n",
    "        torch.save(model_weights, params[\"weights_path\"])\n",
    "\n",
    "\n",
    "def run():\n",
    "    agent1 = DQNAgent(params=params1)\n",
    "    agent1.optimizer = optim.Adam(\n",
    "        agent1.parameters(), weight_decay=0, lr=params1['learning_rate'])\n",
    "    agent2 = DQNAgent(params=params2)\n",
    "    agent2.optimizer = optim.Adam(\n",
    "        agent2.parameters(), weight_decay=0, lr=params2['learning_rate'])\n",
    "    games_count = 0\n",
    "    steps = 0\n",
    "    while games_count < params['episodes']:\n",
    "        if game.complete:\n",
    "            steps = 0\n",
    "            initialize_game(game=game, players=[player1, player2])\n",
    "            print(\"\\nhands\")\n",
    "\n",
    "            print(to_cs(player1.hand))\n",
    "            print(to_cs(player2.hand))\n",
    "\n",
    "            print(\"\\n top card\")\n",
    "            print(cs[game.top_card])\n",
    "\n",
    "        if game.turn == 0:\n",
    "            if not params1['train']:\n",
    "                agent1.epsilon = 0.01\n",
    "            else:\n",
    "                agent1.epsilon = 1 - (steps * params1[\"epsilon_decay_linear\"])\n",
    "\n",
    "            play(player=player1, game=game, agent=agent1,\n",
    "                 params=params1, opponent=player2, opp_agent=agent2)\n",
    "        elif game.turn == 1:\n",
    "            if not params2['train']:\n",
    "                agent2.epsilon = 0.01\n",
    "            else:\n",
    "                agent2.epsilon = 1 - \\\n",
    "                    (games_count * params1[\"epsilon_decay_linear\"])\n",
    "            play(player=player2, game=game, agent=agent2,\n",
    "                 params=params2, opponent=player1, opp_agent=agent1)\n",
    "\n",
    "        steps += 1\n",
    "        if game.complete:\n",
    "            games_count += 1\n",
    "\n",
    "        print(f\"game: {games_count}.  step: {steps} turn: {game.turn}\")\n",
    "\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b312e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimkeys for mr. py",
   "language": "python",
   "name": "aimkeysmachine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
